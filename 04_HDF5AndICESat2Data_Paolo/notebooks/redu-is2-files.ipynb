{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing ICESat-2 data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Select files of interest (segment and time)\n",
    "* Select area of interest (subset lon/lat)\n",
    "* Reduce selected files with variables of interest\n",
    "* Filter data and separate tracks into asc/des\n",
    "* Process/Read each file in parallel\n",
    "* Plot some data to check everything went well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How ICESat-2 files are organized \n",
    "\n",
    "ICESat-2 ground tracks are subsetted into granules (individual files)\n",
    "\n",
    "Granules are then grouped into latitudinal bands (segments)\n",
    "\n",
    "![Segments](segments.png \"Latitudinal bands (Segments)\")\n",
    "\n",
    "\n",
    "File naming convention:\n",
    "\n",
    "`ATL06_20181120202321_08130101_001_01.h5`\n",
    "\n",
    "`ATL06_[yyyymmdd][hhmmss]_[RGTccss]_[vvv_rr].h5`\n",
    "\n",
    "where\n",
    "\n",
    "`ATL_06` => L3A Land Ice product    \n",
    "\n",
    "`yyyymmdd` => Year, month, day of data acquisition    \n",
    "\n",
    "`hhmmss` => Hour, minute, second of data acquisition   \n",
    "\n",
    "`RGT` => Reference Ground Track    \n",
    "\n",
    "`cc` => Cycle Number   \n",
    "\n",
    "`ss` => Segment number (latitude band)   \n",
    "\n",
    "`vvv_rr` => Version and revision numbers  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select files of interest\n",
    "\n",
    "We will use the **file name** info for this (no need to open the files).  \n",
    "\n",
    "Alternatively, we could retrieve this info from the **Metadata**.\n",
    "\n",
    "To select files withint a time interval and segment, all we need is:\n",
    "\n",
    "`yyyymmdd, hhmmss, ss`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's firt get a list with all file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "\n",
    "def list_files_local(path):\n",
    "    \"\"\" Get file list form local folder. \"\"\"\n",
    "    from glob import glob\n",
    "    return glob(path)\n",
    "\n",
    "\n",
    "def list_files_s3(path, bucket):\n",
    "    \"\"\" Get file list from Amazon S3. \"\"\"\n",
    "    import s3fs\n",
    "    return s3fs.S3FileSystem().glob(path, bucket)\n",
    "\n",
    "\n",
    "def list_files_ssh(path, host, user, pwd):\n",
    "    \"\"\" Get file list from remote folder usgin SSH. \"\"\"\n",
    "    import paramiko\n",
    "\n",
    "    # Create an SSH client instance.\n",
    "    ssh = paramiko.SSHClient()\n",
    "\n",
    "    # Create a 'host_keys' object\n",
    "    # and load the local known hosts  \n",
    "    host_keys = ssh.load_system_host_keys()\n",
    "\n",
    "    # Connect to our client w/remote machine credentials\n",
    "    ssh.connect(host, username=user, password=pwd)\n",
    "\n",
    "    # Execute command on remote system,\n",
    "    # and get input, output and error variables\n",
    "    stdin, stdout, stderr = ssh.exec_command('ls '+path)\n",
    "\n",
    "    # Iterate over stdout\n",
    "    files = [line.strip('\\n') for line in stdout]\n",
    "\n",
    "    # Close the connection to client\n",
    "    ssh.close()\n",
    "    return files\n",
    "    \n",
    "\n",
    "if 1:\n",
    "    path = 'data/*.h5'\n",
    "    files = list_files_local(path)\n",
    "    #files = !ls data/*.h5  # alternatively\n",
    "    \n",
    "elif 1:\n",
    "    #path = 's3://pangeo-data-upload-oregon/icesat2/atl06/*.h5'\n",
    "    path = 's3://pangeo-data-upload-oregon/icesat2/data-access-outputs/*.h5'\n",
    "    bucket = 'pangeo-data-upload-oregon'\n",
    "    files = list_files_s3(path, bucket)\n",
    "       \n",
    "else:    \n",
    "    path = '/u/devon-r2/shared_data/icesat2/atl06/rel209/raw/*.h5'\n",
    "    host = 'devon.jpl.nasa.gov'\n",
    "    user = 'paolofer'\n",
    "    pwd = getpass.getpass('Password:')\n",
    "    files = list_files_ssh(path, host, user, pwd)\n",
    "    \n",
    "\n",
    "for f in files[:5]: print(f)\n",
    "print('Total number of files:', len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter file names by segment and time interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "# File name format: ATL06_[yyyymmdd][hhmmss]_[RGTccss]_[vvv_rr].h5\n",
    "\n",
    "#NOTE: Need to simplify this function\n",
    "def time_from_fname(fname):\n",
    "    \"\"\" IS2 fname -> datatime object. \"\"\"\n",
    "    t = fname.split('_')[1]\n",
    "    y, m , d, h, mn, s = t[:4], t[4:6], t[6:8], t[8:10], t[10:12], t[12:14]\n",
    "    time = dt.datetime(int(y), int(m), int(d), int(h), int(mn), int(s))\n",
    "    return time\n",
    "\n",
    "\n",
    "def segment_from_fname(fname):\n",
    "    \"\"\" IS2 fname -> segment number. \"\"\"\n",
    "    s = fname.split('_')[2]\n",
    "    return int(s[-2:])\n",
    "\n",
    "\n",
    "def select_files(files, segments=[10,11,12], t1=(2019,1,1), t2=(2019,2,1)):\n",
    "    t1 = dt.datetime(*t1)\n",
    "    t2 = dt.datetime(*t2)\n",
    "    files_out = []\n",
    "    for f in files:\n",
    "        fname = os.path.basename(f)\n",
    "        time = time_from_fname(fname)\n",
    "        segment = segment_from_fname(fname)\n",
    "        if t1 <= time <= t2 and segment in segments:\n",
    "            files_out.append(f)\n",
    "    return files_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update file list\n",
    "files = select_files(files, segments=[10,11,12], t1=(2019,1,1), t2=(2019,2,1))\n",
    "\n",
    "for f in files[:5]: print(f)\n",
    "print('Number of files:', len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download files of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Tip!** You can download the entire folder from S3 using the `aws client`.\n",
    "\n",
    "Install and setup `aws client`:  \n",
    "https://www.viget.com/articles/set-up-aws-cli-and-download-your-s3-files-from-the-command-line/  \n",
    "\n",
    "List, download and upload folder content from the terminal:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls s3://pangeo-data-upload-oregon/icesat2/                              # list folder content\n",
    "    \n",
    "#!aws s3 sync s3://pangeo-data-upload-oregon/icesat2/data-access-outputs/ data/                     # download folder\n",
    "\n",
    "#!aws --no-sign-request s3 sync data/ s3://pangeo-data-upload-oregon/icesat2/data-access-outputs/   # upload folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download selected files with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "\n",
    "def get_files_s3(files, opath, bucket='pangeo-data-upload-oregon'):\n",
    "    \"\"\" Download files from Amazon S3. \"\"\"\n",
    "    import os\n",
    "    import s3fs\n",
    "    os.chdir(opath)  # change current dir\n",
    "    for f in files: \n",
    "        print(f)\n",
    "        s3fs.S3FileSystem().get(f, bucket)\n",
    "    os.chdir('../')\n",
    "    print('files saved ->', opath)\n",
    "    \n",
    "        \n",
    "def get_files_ssh(files, opath, host, user, pwd):      \n",
    "    import os\n",
    "    files = ' '.join(files)\n",
    "    os.system(\"rsync -Pav '%s@%s:%s' %s\" % (user, host, files, opath))\n",
    "    print('files saved ->', opath)\n",
    "    \n",
    "\n",
    "if 1:\n",
    "    # We already downloaded the files for you -> /data\n",
    "    pass\n",
    "\n",
    "elif 0:\n",
    "    get_files_s3(files, '/home/jovyan/intro-hdf5/notebooks/data/')\n",
    "    \n",
    "else:\n",
    "    opath = 'data/'\n",
    "    host = 'devon.jpl.nasa.gov'\n",
    "    user = 'paolofer'\n",
    "    pwd = getpass.getpass('Password:')\n",
    "    get_files_ssh(files[:3], opath, host, user, pwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing ICESat-2 files\n",
    "\n",
    "***\n",
    "**NOTE:** \n",
    "\n",
    "+ **This is neither the only nor the best way to handled ICESat-2 data files.**\n",
    "\n",
    "+ **This is *one* way that works well for large-scale processing (e.g. full continent) on parallel machines (e.g. HPC clusters).**\n",
    "\n",
    "+ **The idea is to (a) simplify the I/O of a complex workflow and (b) take advantage of embarrasingly parallelization.**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the ICESat-2 file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm data/*_gt*\n",
    "!ls data/*.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!h5ls -r data/ATL06_20190101001723_00540210_209_01.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code a simple reader that:\n",
    "\n",
    "- Select variables of interest (x, y, t, h...)  \n",
    "- Filter data points based on quality flag and bbox   \n",
    "- Separate into beams and ascending/descending tracks  \n",
    "- Save data to a simpler HDF5 structure (redundancy/efficiency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyproj\n",
    "from astropy.time import Time\n",
    "\n",
    "def gps2dyr(time):\n",
    "    \"\"\" Converte GPS time to decimal years. \"\"\"\n",
    "    return Time(time, format='gps').decimalyear\n",
    "\n",
    "\n",
    "def track_type(time, lat, tmax=1):\n",
    "    \"\"\"\n",
    "    Separate tracks into ascending and descending.\n",
    "    \n",
    "    Defines tracks as segments with time breaks > tmax,\n",
    "    and tests whether lat increases or decreases w/time.\n",
    "    \"\"\"\n",
    "    tracks = np.zeros(lat.shape)  # generate track segment\n",
    "    tracks[0:np.argmax(np.abs(lat))] = 1  # set values for segment\n",
    "    i_asc = np.zeros(tracks.shape, dtype=bool)  # output index array\n",
    "\n",
    "    # Loop trough individual secments\n",
    "    for track in np.unique(tracks):\n",
    "    \n",
    "        i_track, = np.where(track == tracks)  # get all pts from seg\n",
    "    \n",
    "        if len(i_track) < 2: continue\n",
    "    \n",
    "        # Test if lat increases (asc) or decreases (des) w/time\n",
    "        i_min = time[i_track].argmin()\n",
    "        i_max = time[i_track].argmax()\n",
    "        lat_diff = lat[i_track][i_max] - lat[i_track][i_min]\n",
    "    \n",
    "        # Determine track type\n",
    "        if lat_diff > 0:  i_asc[i_track] = True\n",
    "    \n",
    "    return i_asc, np.invert(i_asc)  # index vectors\n",
    "\n",
    "\n",
    "def transform_coord(proj1, proj2, x, y):\n",
    "    \"\"\"\n",
    "    Transform coordinates from proj1 to proj2 (EPSG num).\n",
    "\n",
    "    Example EPSG projs:\n",
    "        Geodetic (lon/lat): 4326\n",
    "        Polar Stereo AnIS (x/y): 3031\n",
    "        Polar Stereo GrIS (x/y): 3413\n",
    "    \"\"\"\n",
    "    # Set full EPSG projection strings\n",
    "    proj1 = pyproj.Proj(\"+init=EPSG:\"+str(proj1))\n",
    "    proj2 = pyproj.Proj(\"+init=EPSG:\"+str(proj2))\n",
    "    return pyproj.transform(proj1, proj2, x, y)  # convert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def read_atl06(fname, bbox=None):\n",
    "    \"\"\" \n",
    "    Read 1 ATL06 file and output 6 reduced files. \n",
    "    \n",
    "    Extract variables of interest and separate the ATL06 file \n",
    "    into each beam (ground track) and ascending/descending orbits.\n",
    "    \"\"\"\n",
    "\n",
    "    # Each beam is a group\n",
    "    group = ['/gt1l', '/gt1r', '/gt2l', '/gt2r', '/gt3l', '/gt3r']\n",
    "\n",
    "    # Loop trough beams\n",
    "    for k,g in enumerate(group):\n",
    "    \n",
    "        #-----------------------------------#\n",
    "        # 1) Read in data for a single beam #\n",
    "        #-----------------------------------#\n",
    "    \n",
    "        # Load variables into memory (more can be added!)\n",
    "        with h5py.File(fname, 'r') as fi:\n",
    "            lat = fi[g+'/land_ice_segments/latitude'][:]\n",
    "            lon = fi[g+'/land_ice_segments/longitude'][:]\n",
    "            h_li = fi[g+'/land_ice_segments/h_li'][:]\n",
    "            s_li = fi[g+'/land_ice_segments/h_li_sigma'][:]\n",
    "            t_dt = fi[g+'/land_ice_segments/delta_time'][:]\n",
    "            q_flag = fi[g+'/land_ice_segments/atl06_quality_summary'][:]\n",
    "            s_fg = fi[g+'/land_ice_segments/fit_statistics/signal_selection_source'][:]\n",
    "            snr = fi[g+'/land_ice_segments/fit_statistics/snr_significance'][:]\n",
    "            h_rb = fi[g+'/land_ice_segments/fit_statistics/h_robust_sprd'][:]\n",
    "            dac = fi[g+'/land_ice_segments/geophysical/dac'][:]\n",
    "            f_sn = fi[g+'/land_ice_segments/geophysical/bsnow_conf'][:]\n",
    "            dh_fit_dx = fi[g+'/land_ice_segments/fit_statistics/dh_fit_dx'][:]\n",
    "            tide_earth = fi[g+'/land_ice_segments/geophysical/tide_earth'][:]\n",
    "            tide_load = fi[g+'/land_ice_segments/geophysical/tide_load'][:]\n",
    "            tide_ocean = fi[g+'/land_ice_segments/geophysical/tide_ocean'][:]\n",
    "            tide_pole = fi[g+'/land_ice_segments/geophysical/tide_pole'][:]\n",
    "            t_ref = fi['/ancillary_data/atlas_sdp_gps_epoch'][:]\n",
    "            rgt = fi['/orbit_info/rgt'][:] * np.ones(len(lat))\n",
    "            orb = np.full_like(h_li, k)\n",
    "\n",
    "        #---------------------------------------------#\n",
    "        # 2) Filter data according region and quality #\n",
    "        #---------------------------------------------#\n",
    "        \n",
    "        # Select a region of interest\n",
    "        if bbox:\n",
    "            lonmin, lonmax, latmin, latmax = bbox\n",
    "            bbox_mask = (lon >= lonmin) & (lon <= lonmax) & \\\n",
    "                        (lat >= latmin) & (lat <= latmax)\n",
    "        else:\n",
    "            bbox_mask = np.ones_like(lat, dtype=bool)  # get all\n",
    "            \n",
    "        # Only keep good data, and data inside bbox\n",
    "        mask = (q_flag == 0) & (np.abs(h_li) < 10e3) & (bbox_mask == 1)\n",
    "        \n",
    "        # Update variables\n",
    "        lat, lon, h_li, s_li, t_dt, h_rb, s_fg, snr, q_flag, f_sn, \\\n",
    "            tide_earth, tide_load, tide_ocean, tide_pole, dac, rgt, orb = \\\n",
    "                lat[mask], lon[mask], h_li[mask], s_li[mask], t_dt[mask], \\\n",
    "                h_rb[mask], s_fg[mask], snr[mask], q_flag[mask], f_sn[mask], \\\n",
    "                tide_earth[mask], tide_load[mask], tide_ocean[mask], \\\n",
    "                tide_pole[mask], dac[mask], rgt[mask], orb[mask]\n",
    "\n",
    "        # Test for no data\n",
    "        if len(h_li) == 0: continue\n",
    "\n",
    "        #-------------------------------------#\n",
    "        # 3) Convert time and separate tracks #\n",
    "        #-------------------------------------#\n",
    "        \n",
    "        # Time in GPS seconds (secs sinde 1980...)\n",
    "        t_gps = t_ref + t_dt\n",
    "\n",
    "        # Time in decimal years\n",
    "        t_year = gps2dyr(t_gps)\n",
    "\n",
    "        # Determine orbit type\n",
    "        i_asc, i_des = track_type(t_year, lat)\n",
    "        \n",
    "        #-----------------------#\n",
    "        # 4) Save selected data #\n",
    "        #-----------------------#\n",
    "        \n",
    "        # Define output file name\n",
    "        ofile = fname.replace('.h5', '_'+g[1:]+'.h5')\n",
    "                \n",
    "        # Save variables\n",
    "        with h5py.File(ofile, 'w') as f:\n",
    "            f['orbit'] = orb\n",
    "            f['lon'] = lon\n",
    "            f['lat'] = lat\n",
    "            f['h_elv'] = h_li\n",
    "            f['t_year'] = t_year\n",
    "            f['t_sec'] = t_gps\n",
    "            f['s_elv'] = s_li\n",
    "            f['h_rb'] = h_rb\n",
    "            f['s_fg'] = s_fg\n",
    "            f['snr'] = snr\n",
    "            f['q_flg'] = q_flag\n",
    "            f['f_sn'] = f_sn\n",
    "            f['tide_load'] = tide_load\n",
    "            f['tide_ocean'] = tide_ocean\n",
    "            f['tide_pole'] = tide_pole\n",
    "            f['tide_earth'] = tide_earth\n",
    "            f['dac'] = dac\n",
    "            f['rgt'] = rgt\n",
    "            f['trk_type'] = i_asc\n",
    "\n",
    "            print('out ->', ofile)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple parallelization\n",
    "\n",
    "* If your problem is embarrasingly parallel, it's easy to parallelize\n",
    "* We can use the very simple and lightweight `joblib` library\n",
    "* There is no need to modify your code!\n",
    "\n",
    "Read more: [https://joblib.readthedocs.io](https://joblib.readthedocs.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Let's check the available resources first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "njobs = 3\n",
    "\n",
    "#files = list_files_local('data/*_01.h5')  # we already created our file list\n",
    "\n",
    "bbox = None #[-1124782, 81623, -919821, -96334]  # Kamb bounding box\n",
    "\n",
    "if njobs == 1:\n",
    "    print('running in serial ...')\n",
    "    [read_atl06(f, bbox) for f in files]\n",
    "\n",
    "else:\n",
    "    print('running in parallel (%d jobs) ...' % njobs)\n",
    "    from joblib import Parallel, delayed\n",
    "    Parallel(n_jobs=njobs, verbose=5)(delayed(read_atl06)(f, bbox) for f in files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our created files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/*.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!h5ls -r data/ATL06_20190101001723_00540210_209_01_gt1l.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot some data to check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot track points within region to check\n",
    "* Plot track/beam profiles to check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data now becomes trivial!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_h5(fname, vnames=[]):\n",
    "    \"\"\" Simple HDF5 reader. \"\"\"\n",
    "    with h5py.File(fname, 'r') as f:\n",
    "        return [f[v][:] for v in vnames]\n",
    "\n",
    "    \n",
    "files = list_files_local('data/ATL06_20190101002504_00540211_209_01_gt*')\n",
    "\n",
    "fig = plt.figure(figsize=(14,5))\n",
    "ax1 = plt.subplot(121)\n",
    "ax2 = plt.subplot(122)\n",
    "\n",
    "for fname in files:\n",
    "\n",
    "    lon, lat, t, h = read_h5(fname, ['lon', 'lat', 't_year', 'h_elv'])\n",
    "    \n",
    "    x, y = transform_coord(4326, 3031, lon, lat)\n",
    " \n",
    "    ax1.plot(x, y, '.', label='%s' % fname[-7:-3])\n",
    "    ax2.plot(t, h, '.', label='%s' % fname[-7:-3])\n",
    "    \n",
    "ax1.set_xlabel('x (m)')\n",
    "ax1.set_ylabel('y (m)')\n",
    "ax1.legend()\n",
    "ax2.set_xlabel('time (yr)')\n",
    "ax2.set_ylabel('height (m)')\n",
    "ax2.legend()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single program from the command line\n",
    "\n",
    "You can put all of the above (and more) into a single script and run it on the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python readatl06.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try reading the ICESat-2 files in parallel from the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python readatl06.py data/*_01.h5 -n 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat readatl06.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
